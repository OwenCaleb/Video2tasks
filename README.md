# Robot Video Segmentor

A distributed video segmentation system for robotic manipulation tasks using Vision-Language Models (VLMs).

## Overview

This project provides a client-server architecture for analyzing robot videos and detecting task boundaries (switch points) using VLMs like Qwen3-VL. The system:

- **Server**: Manages job queues, video frame extraction, and result aggregation
- **Worker**: Runs VLM inference to detect task transitions in video windows

## Features

- ğŸ¥ Video window sampling with configurable parameters
- ğŸ¤– Pluggable VLM backends (Qwen3-VL, or custom implementations)
- ğŸ“Š Automatic segment generation from VLM outputs
- ğŸ”„ Distributed processing support (multiple workers)
- âš™ï¸ YAML-based configuration
- ğŸ–¥ï¸ Cross-platform support (Linux/GPU recommended, Windows/CPU with dummy backend)

## Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/robot-video-segmentor.git
cd robot-video-segmentor

# Install with core dependencies
pip install -e .

# Or install with Qwen3-VL support (requires GPU)
pip install -e ".[qwen3vl]"
```

### Configuration

Copy the example configuration and customize:

```bash
cp config.example.yaml config.yaml
# Edit config.yaml with your paths and settings
```

### Running

Start the server:
```bash
rvs-server --config config.yaml
```

Start a worker (in another terminal):
```bash
rvs-worker --config config.yaml
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Server    â”‚â”€â”€â”€â”€â–¶â”‚  Job Queue  â”‚â—€â”€â”€â”€â”€â”‚   Worker    â”‚
â”‚  (FastAPI)  â”‚     â”‚             â”‚     â”‚  (VLM)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                    â”‚
       â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Video Files â”‚                       â”‚  VLM Model  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration

See `config.example.yaml` for all available options:

- **datasets**: Video dataset paths and subsets
- **run**: Output directory configuration
- **server**: Host, port, and queue settings
- **worker**: VLM backend selection and model paths
- **windowing**: Frame sampling parameters

## VLM Backends

### Dummy Backend (Default)
Lightweight backend for testing and Windows/CPU environments. Returns mock results without loading heavy models.

### Qwen3-VL Backend
Full inference using Qwen3-VL-32B-Instruct (or other Qwen3-VL variants). Requires:
- Linux with NVIDIA GPU
- 24GB+ VRAM recommended for 32B model
- PyTorch with CUDA support

### Custom Backend
Implement the `VLMBackend` interface to add your own VLM:

```python
from robot_video_segmentor.vlm.base import VLMBackend

class MyBackend(VLMBackend):
    def infer(self, images, prompt):
        # Your inference logic
        return {"transitions": [], "instructions": []}
```

## Development

### Project Structure

```
robot-video-segmentor/
â”œâ”€â”€ src/robot_video_segmentor/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration models
â”‚   â”œâ”€â”€ server/            # FastAPI server
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ windowing.py
â”‚   â”œâ”€â”€ worker/            # Worker implementation
â”‚   â”‚   â”œâ”€â”€ runner.py
â”‚   â”‚   â””â”€â”€ backends/
â”‚   â”‚       â”œâ”€â”€ dummy.py
â”‚   â”‚       â””â”€â”€ qwen3vl.py
â”‚   â””â”€â”€ cli/               # CLI entrypoints
â”‚       â”œâ”€â”€ server.py
â”‚       â””â”€â”€ worker.py
â”œâ”€â”€ config.example.yaml
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

### Testing

```bash
# Validate configuration
python -m robot_video_segmentor.validate_config --config config.yaml

# Run tests
pytest
```

## Requirements

### Minimum (Dummy Backend)
- Python 3.8+
- 4GB RAM
- Any OS (Windows/Linux/macOS)

### Recommended (Qwen3-VL Backend)
- Python 3.8+
- Linux with NVIDIA GPU
- 24GB+ VRAM
- CUDA 11.8+ / 12.x

## License

MIT License - see [LICENSE](LICENSE) file for details.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/)
- VLM support via [Transformers](https://huggingface.co/docs/transformers/)
- Inspired by robotic video analysis research