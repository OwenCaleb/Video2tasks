<div align="center">

# ğŸ¤– Robot Video Segmentor

**A distributed video segmentation system for robotic manipulation tasks using Vision-Language Models**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![FastAPI](https://img.shields.io/badge/FastAPI-009688?logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com/)

[English](README.md) | [ä¸­æ–‡æ–‡æ¡£](README_CN.md)

</div>

---

## ğŸ“– Overview

Robot Video Segmentor provides a **client-server architecture** for analyzing robot videos and detecting task boundaries (switch points) using VLMs like Qwen3-VL.

| Component | Description |
|-----------|-------------|
| **Server** | Manages job queues, video frame extraction, and result aggregation |
| **Worker** | Runs VLM inference to detect task transitions in video windows |

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ¥ **Video Windowing** | Configurable video window sampling parameters |
| ğŸ¤– **Pluggable Backends** | Support for Qwen3-VL, Remote API, or custom VLM implementations |
| ğŸ“Š **Smart Aggregation** | Automatic segment generation with weighted voting & Hanning window |
| ğŸ”„ **Distributed Processing** | Scale horizontally with multiple workers |
| âš™ï¸ **YAML Config** | Simple, declarative configuration management |
| ğŸ–¥ï¸ **Cross-Platform** | Linux/GPU recommended; Windows/CPU with dummy backend |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â”‚     Server      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   Job Queue     â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”‚     Worker      â”‚
â”‚    (FastAPI)    â”‚         â”‚                 â”‚         â”‚     (VLM)       â”‚
â”‚                 â”‚         â”‚                 â”‚         â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                                       â”‚
         â–¼                                                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Video Files   â”‚                                     â”‚    VLM Model    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/ly-geming/robot-video-segmentor.git
cd robot-video-segmentor

# Install with core dependencies
pip install -e .

# Or install with Qwen3-VL support (requires GPU)
pip install -e ".[qwen3vl]"
```

### Configuration

```bash
# Copy example config
cp config.example.yaml config.yaml

# Edit with your paths and settings
vim config.yaml  # or your preferred editor
```

### Running

**Terminal 1 - Start the Server:**
```bash
rvs-server --config config.yaml
```

**Terminal 2 - Start a Worker:**
```bash
rvs-worker --config config.yaml
```

> ğŸ’¡ **Tip:** You can start multiple workers to process videos in parallel!

---

## âš™ï¸ Configuration

See [`config.example.yaml`](config.example.yaml) for all available options:

| Section | Description |
|---------|-------------|
| `datasets` | Video dataset paths and subsets |
| `run` | Output directory configuration |
| `server` | Host, port, and queue settings |
| `worker` | VLM backend selection and model paths |
| `windowing` | Frame sampling parameters |

---

## ğŸ”Œ VLM Backends

### Dummy Backend (Default)

Lightweight backend for testing and Windows/CPU environments. Returns mock results without loading heavy models.

```yaml
worker:
  backend: dummy
```

### Qwen3-VL Backend

Full inference using Qwen3-VL-32B-Instruct (or other variants).

**Requirements:**
- ğŸ§ Linux with NVIDIA GPU
- ğŸ’¾ 24GB+ VRAM (for 32B model)
- ğŸ”¥ PyTorch with CUDA support

```yaml
worker:
  backend: qwen3vl
  model_path: /path/to/model
```

### Remote API Backend

Use an external API endpoint for inference:

```yaml
worker:
  backend: remote_api
  api_url: http://your-api-server/infer
```

<details>
<summary>ğŸ“¡ API Request/Response Format</summary>

**Request:**
```json
{
  "prompt": "...",
  "images_b64_png": ["...", "..."]
}
```

**Response:**
```json
{
  "transitions": [6],
  "instructions": ["Place the fork", "Place the spoon"],
  "thought": "..."
}
```

</details>

### Custom Backend

Implement the `VLMBackend` interface to add your own:

```python
from robot_video_segmentor.vlm.base import VLMBackend

class MyBackend(VLMBackend):
    def infer(self, images, prompt):
        # Your inference logic
        return {"transitions": [], "instructions": []}
```

---

## ğŸ“ Project Structure

```
robot-video-segmentor/
â”œâ”€â”€ ğŸ“‚ src/robot_video_segmentor/
â”‚   â”œâ”€â”€ config.py              # Configuration models
â”‚   â”œâ”€â”€ prompt.py              # Prompt templates
â”‚   â”œâ”€â”€ ğŸ“‚ server/             # FastAPI server
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ windowing.py
â”‚   â”œâ”€â”€ ğŸ“‚ worker/             # Worker implementation
â”‚   â”‚   â””â”€â”€ runner.py
â”‚   â”œâ”€â”€ ğŸ“‚ vlm/                # VLM backends
â”‚   â”‚   â”œâ”€â”€ dummy.py
â”‚   â”‚   â”œâ”€â”€ qwen3vl.py
â”‚   â”‚   â””â”€â”€ remote_api.py
â”‚   â””â”€â”€ ğŸ“‚ cli/                # CLI entrypoints
â”‚       â”œâ”€â”€ server.py
â”‚       â””â”€â”€ worker.py
â”œâ”€â”€ ğŸ“„ config.example.yaml
â”œâ”€â”€ ğŸ“„ pyproject.toml
â”œâ”€â”€ ğŸ“„ README.md
â”œâ”€â”€ ğŸ“„ README_CN.md
â””â”€â”€ ğŸ“„ LICENSE
```

---

## ğŸ§ª Testing

```bash
# Validate configuration
rvs-validate-config --config config.yaml

# Run tests
pytest
```

---

## ğŸ’» Requirements

<table>
<tr>
<th>Minimum (Dummy Backend)</th>
<th>Recommended (Qwen3-VL)</th>
</tr>
<tr>
<td>

- Python 3.8+
- 4GB RAM
- Any OS

</td>
<td>

- Python 3.8+
- Linux + NVIDIA GPU
- 24GB+ VRAM
- CUDA 11.8+ / 12.x

</td>
</tr>
</table>

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Built with [FastAPI](https://fastapi.tiangolo.com/)
- VLM support via [Transformers](https://huggingface.co/docs/transformers/)
- Inspired by robotic video analysis research

---

<div align="center">

**â­ Star this repo if you find it useful! â­**

</div>
